---
comments: true
---

# [ICLR 2024] Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs

???+ info

    - Author: Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi
    - Conference: ICLR 2024
    - arXiv: [2306.13063](https://arxiv.org/abs/2306.13063)

<div class="card file-block" markdown="1">
<div class="file-icon"><img src="/Note/assets/images/icons/pdf.svg" style="height: 3em;"></div>
<div class="file-body">
<div class="file-title">论文</div>
<div class="file-meta">1.15 MB / 29 P / 2025-02-25</div>
</div>
<a class="down-button" target="_blank" href="/Note/assets/files/science_research/arXiv_2306_13063.pdf" markdown="1">:fontawesome-solid-download: 下载</a>
</div>

这篇论文的核心研究问题是**如何让大语言模型（LLMs）准确表达它们对自身回答的信心（confidence）**，即**置信度提取（confidence elicitation）**。研究的动机在于，如果 LLMs 能够准确评估自身回答的可靠性，将极大提升它们在实际应用中的可信度，减少错误信息带来的风险。

## 研究背景与动机

- 传统的置信度评估方法通常依赖于**白盒（white-box）方法**，即直接访问模型内部信息（如 token 概率）或通过微调（fine-tuning）进行校准。
- 但是，随着**封闭式 LLM（如 GPT-4）**的兴起，这些方法不再适用，因为用户无法访问模型的内部信息，也无法进行微调。因此，**探索黑盒（black-box）方法**成为研究重点。
- 黑盒方法指的是仅通过模型的输入输出行为来推测其不确定性，而不需要内部访问权限。

## 提出的黑盒置信度提取框架

论文提出了一个系统性的**黑盒置信度提取框架**，包括以下三个关键组件：

1. **提示策略（Prompting Strategy）**
      - 让 LLM **「口头表达」** 其信心（verbalized confidence），比如在回答后补充「我对这个答案的置信度是 85%」。
      - 采用不同的提示方式，如：
          - **基础提示（Vanilla）**：直接让模型输出答案和信心。
          - **思维链提示（CoT, Chain of Thought）**：让模型先推理，再表达信心。
          - **自我检验（Self-Probing）**：给定问题和潜在答案后，问模型「这个答案的正确概率是多少？」。
          - **多步提示（Multi-Step）**：让模型逐步推理，并对每一步进行信心评估。
          - **Top-K 提示**：让模型输出 K 个最可能的答案，并对每个答案打分。
2. **采样策略（Sampling Strategy）**
      - 通过生成多个答案来评估 LLM 的稳定性，例如：
          - **自随机采样（Self-Random）**：在相同提示下多次采样，观察回答的一致性。
          - **重述问题（Prompting）**：用不同的表述方式询问相同问题，看看模型是否一致。
          - **误导性信息（Misleading）**：在问题中加入错误引导，观察模型是否改变答案或信心。
3. **聚合策略（Aggregation Strategy）**
      - 通过不同方式整合多个回答的置信度，例如：
          - **一致性方法（Consistency）**：如果多个采样的答案相同，说明置信度较高。
          - **平均置信度（Avg-Conf）**：计算所有 verbalized confidence 的加权平均值。
          - **Top-K 排序方法（Pair-Rank）**：使用模型对答案排序的稳定性来估算置信度。

## 主要实验和分析

- 论文在两个主要任务上测试了框架的有效性：
    1. **置信度校准（Confidence Calibration）**：衡量模型表达的置信度是否与其实际准确率相匹配（例如，90% 置信度的答案是否真的有 90% 的正确率）。
    2. **错误预测（Failure Prediction）**：衡量模型是否能用置信度来区分正确和错误的回答。
- **数据集**：论文使用了 5 种类型的数据集，包括：
    - **常识推理**（Commonsense Reasoning）
    - **数学推理**（Arithmetic Reasoning）
    - **符号推理**（Symbolic Reasoning）
    - **伦理问题**（Ethics）
    - **专业知识问题**（Professional Knowledge）
- **测试的 LLMs**：GPT-3, GPT-3.5, GPT-4, Vicuna, LLaMA 2 Chat。
- **关键实验结论**
    1. LLMs 在表达置信度时**普遍过度自信（overconfident）**，特别是大多数 verbalized confidence 都在 80%-100% 之间，并且通常是 5 的倍数（如 85%、90%）。
    2. 随着模型能力的提升，校准和错误预测性能有所提高，但仍然远非理想。
    3. **人类启发的提示策略（如 CoT、自我检验）可以缓解过度自信**，但在 GPT-4 这样的高级模型上改进效果会减弱。
    4. **采样策略（生成多个答案）可以显著提升错误预测能力**，特别是数学类任务（如 GSM8K）。
    5. **聚合策略的选择对不同任务的影响不同**：
        - **Pair-Rank 更适合置信度校准**（ECE 误差更低）。
        - **Avg-Conf 更适合错误预测**（AUROC 提高）。

## 黑盒方法与白盒方法的比较

- 论文还对**白盒方法（依赖 token 概率）**与黑盒方法进行了对比，发现：
    - 白盒方法在置信度校准和错误预测上表现稍好，但优势不明显（例如 AUROC 仅从 0.522 提高到 0.605）。
    - 由于 LLM 主要基于 token 级别的概率进行预测，白盒方法难以捕捉语义层面的不确定性，因此仍然有改进空间。

## 未来研究方向

- 论文指出当前的黑盒方法仍然存在不足，特别是在**处理专业知识任务**上（如法律、医学）。
- 未来的研究可以探索：
    1. **结合白盒与黑盒方法**，例如利用少量的内部模型信息来增强黑盒置信度提取的能力。
    2. **扩展到开放式任务**，如摘要、开放式问答，而不仅仅是固定答案的问题。
    3. **改进提示策略**，让 LLM 通过更细粒度的推理表达自身的不确定性。

## 总结

这篇论文系统性地研究了如何在黑盒环境下让 LLM 以可靠的方式表达自己的不确定性。研究结果表明，当前 LLMs **在 verbalized confidence 上存在严重的过度自信问题**，但可以通过**提示、采样和聚合策略**部分缓解。此外，**虽然白盒方法稍优，但黑盒方法的研究仍然具有很大价值**，尤其是在封闭式 LLM 环境（如 GPT-4）下。未来的改进方向包括结合白盒信息、优化提示策略和拓展适用任务类型。
