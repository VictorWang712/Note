---
comments: true
---

# [ICSE] Calibration and Correctness of Language Models for Code

???+ info

    - Author: Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md Rafiqul Islam Rabin, Amin Alipour, Susmit Jha, Prem Devanbu, Toufique Ahmed
    - Conference: ICSE 2025
    - arXiv: [2402.02047](https://arxiv.org/abs/2402.02047) 

<div class="card file-block" markdown="1">
<div class="file-icon"><img src="/Note/assets/images/icons/pdf.svg" style="height: 3em;"></div>
<div class="file-body">
<div class="file-title">论文</div>
<div class="file-meta">1.09 MB / 22 P / 2025-03-05</div>
</div>
<a class="down-button" target="_blank" href="/Note/assets/files/science_research/arXiv_2402_02047.pdf" markdown="1">:fontawesome-solid-download: 下载</a>
</div>

## 论文概述

该研究探讨了**大语言模型（LLM, Large Language Model）**在**代码生成任务**（如代码补全、代码合成、程序修复）中的置信度校准（Calibration）问题。研究发现，当前主流 LLM 生成代码时提供的置信度通常与代码的实际正确性存在较大偏差，即**LLM 并不擅长自我评估生成代码的正确性**。

## 研究动机

1. **代码 LLM 的广泛应用**：
    - 代码 LLM（如 GitHub Copilot、GPT-3.5）已广泛用于软件开发，但可能生成**错误代码**，甚至包含**安全漏洞**。
    - 开发者需要可靠的指标来判断 LLM 生成代码的可信度，以决定是否采用。

2. **置信度校准的重要性**：
    - 置信度校准源自天气预报领域，**理想情况下，模型的置信度应与实际正确率匹配**。
    - **良好校准的置信度**有助于开发者更理性地管理代码质量，减少错误代码的无意采纳，同时避免因过度谨慎而错失有用代码。

## 研究问题

1. LLM 置信度是否与代码的实际正确性对齐？
2. 能否通过置信度缩放（Rescaling）改善 LLM 的置信度校准？
3. LLM 能否通过反思（Reflection）更好地评估自己代码的正确性？
4. Few-shot 学习能否提升 LLM 在代码补全任务中的置信度校准？

## 关键概念

### 置信度校准（Confidence Calibration）

如果一个 LLM 在 80% 置信度下生成的代码，实际只有 50% 是正确的，那它就是**过度自信（Overconfidence）**。

如果在 30% 置信度下生成的代码，实际有 70% 是正确的，那它就是**过度谨慎（Underconfidence）**。

数学定义：

$$P(\hat{Y} = Y | P_M(\hat{Y} | X) = p) = p, \forall p \in [0,1]$$

其中：

- $P_M(\hat{Y} | X)$ 是 LLM 生成代码的置信度
- $\hat{Y}$ 是模型预测的代码
- $Y$ 是正确代码

### 校准度量方法

1. **Brier Score（布里尔分数）**
    - $Br = \frac{1}{N} \sum_{i=1}^{N} (p_i - o_i)^2$
    - 衡量模型预测置信度与实际正确率的均方误差，数值越低越好。

2. **Expected Calibration Error（ECE，期望校准误差）**
    - $ECE = \sum_{i=1}^{m} \frac{|B_i|}{|T|} |acc(B_i) - conf(B_i)|$
    - 若 ECE 高，说明 LLM 置信度评估存在较大偏差。

## 研究方法

### 任务与数据集

| 任务 | 数据集 | 样本数 | 正确性判定 |
|:-:|:-:|:-:|:-:|
| **代码合成** | HumanEval, MBPP | 164, 880 | 运行测试用例 |
| **代码补全** | DyPyBench | 1,988 | 运行测试用例 & 精确匹配 |
| **程序修复** | Defects4J, SStubs | 120, 3,000 | 仅精确匹配 |

### 置信度评估方法

- **内在置信度（Intrinsic Confidence）**：LLM 直接给出的置信度，如：
    - 平均 token 置信度（$p_{avg}$）
    - 生成序列总置信度（$p_{tot}$）
- **反思式置信度（Reflective Confidence）**：
    - 让 LLM 以自然语言陈述置信度（Verbalized Self-Ask）
    - 让 LLM 直接回答代码是否正确（T/F Logit）

### 置信度缩放（Confidence Rescaling）

- **Platt Scaling**：通过后处理调整 LLM 置信度，使其更匹配实际正确性。

## 实验结果

1. LLM 置信度是否与代码正确性匹配？
    - **LLM 置信度普遍不准确**，常见 **过度自信或过度谨慎** 现象。
    - ECE 最高可达 0.73，表明 LLM 置信度存在较大偏差。

2. Platt Scaling 是否能改善校准？
    - Platt Scaling **能够降低 Brier Score 和 ECE，提高校准性**，但：
        - 在部分任务中反而**降低准确性**（“桶塌缩”问题）。
        - 需要一定量的正确性数据进行训练，泛化能力有限。

3. LLM 反思是否能提升置信度评估能力？
    - 反思式方法（Reflective Prompts）在**部分任务**中可提高校准性，但整体效果不稳定。
    - 在程序修复任务中，反思方法的 Skill Score 甚至低于基线模型。

4. Few-shot 学习能否提高置信度校准？
    - **BM25 提示优化的 Few-shot 方法可显著提高代码补全任务的置信度校准能力**：
        - Skill Score 从 **0.00 提升至 0.15**。
        - 置信度校准曲线明显改善。

## 结论

1. **LLM 的置信度校准普遍较差**，模型往往过度自信或过度谨慎。
2. **Platt Scaling 可提高校准性**，但存在泛化能力问题。
3. **反思式置信度评估方法在部分任务中有效，但整体表现不稳定**。
4. **Few-shot 学习（特别是 BM25 相关示例检索）可以显著改善代码补全任务的置信度校准**，对代码合成和程序修复任务的提升有限。

## 未来工作

- **探索更稳定的校准方法**，避免“桶塌缩”（在使用 Platt Scaling 或其他后处理方法进行置信度缩放（Rescaling）时。所有样本的置信度值在校准后收敛到一个较小的范围，甚至全部集中在某个固定值上，导致置信度区分能力丧失）现象。
- **开发任务自适应的校准技术**，提高泛化能力。
- **研究 LLM 在不同代码生成任务下的置信度模式**，优化评估框架。
