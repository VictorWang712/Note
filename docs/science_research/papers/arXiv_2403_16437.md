---
comments: true
---

# [ICSE] Reasoning Runtime Behavior of a Program with LLM: How Far Are We?

???+ info

    - Author: Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, Xin Xia
    - Conference: ICSE 2025
    - arXiv: [2403.16437](https://arxiv.org/abs/2403.16437)

<div class="card file-block" markdown="1">
<div class="file-icon"><img src="/Note/assets/images/icons/pdf.svg" style="height: 3em;"></div>
<div class="file-body">
<div class="file-title">论文</div>
<div class="file-meta">983 KB / 13 P / 2025-03-09</div>
</div>
<a class="down-button" target="_blank" href="/Note/assets/files/science_research/arXiv_2403_16437.pdf" markdown="1">:fontawesome-solid-download: 下载</a>
</div>

## 论文概述

本研究评估了**代码大语言模型（Code LLMs）**在**程序运行时行为推理（Runtime Behavior Reasoning, RBR）**任务上的表现。

现有代码基准（如 HumanEval）主要关注输入-输出推理，而忽略了代码执行的**中间行为**及其一致性问题。

论文提出了新的评测框架 **REval**，涵盖：

1. **运行时行为推理（RBR）**：预测代码的执行状态、路径、变量值等。
2. **增量一致性评估（IC）**：衡量 LLM 在相关推理任务中的逻辑一致性。

实验结果显示，**当前 LLM 在 RBR 任务上的平均准确率仅 44.4%，IC 评分仅 10.3**，表明 LLM 在代码执行推理方面存在重大挑战。

## 研究动机

1. **代码 LLM 的应用增长**
    - 代码 LLM（如 CodeLlama, GPT-4-Turbo）在代码生成、漏洞检测等任务中表现出色。
    - 但**代码推理能力仍然较弱**，特别是在理解代码执行行为上。
2. **现有代码评测基准的不足**
    - HumanEval 等基准主要评估 **输入 $\to$ 输出** 关系，而忽略**中间状态**。
    - LLM 可能会给出**不一致的推理结果**（如预测某条语句不会执行，却又认为它修改了变量值）。
3. **代码执行推理的重要性**
    - 代码是可执行的，而不仅仅是文本。
    - **理解执行路径、变量状态和代码覆盖情况**，对**调试、漏洞检测、自动修复**等任务至关重要。

## 研究问题

1. 现有 LLM 是否能正确推理程序运行时行为？
2. LLM 在增量一致性（IC）方面的表现如何？
3. 代码推理能力如何影响代码生成任务？

## 关键概念

### 1. 运行时行为推理（Runtime Behavior Reasoning, RBR）

评估 LLM 是否能推理代码执行的中间状态，包括：

- **代码覆盖预测（CCP）**：某条语句是否会执行？
- **程序状态预测（PSP）**：变量的最终值和类型是什么？
- **执行路径预测（EPP）**：下一条执行的语句是什么？
- **输出预测（OP）**：程序最终的输出是什么？

### 2. 增量一致性评估（Incremental Consistency Evaluation, IC）

- **衡量 LLM 在相互关联的推理任务中的逻辑一致性**。
- 例如，若 LLM 预测某条语句不会执行（CCP），则它不应预测该语句影响变量状态（PSP）。

## 研究方法

### 评测数据集

| 任务 | 数据集 | 样本数 | 评测标准 |
|------|------|------|------|
| **代码合成** | HumanEval, MBPP | 164, 880 | 运行测试用例 |
| **代码补全** | DyPyBench | 1,988 | 运行测试用例 & 精确匹配 |
| **程序修复** | Defects4J, SStubs | 120, 3,000 | 仅精确匹配 |

### 评测指标

- **准确率（Accuracy）**：RBR 任务的主要评测指标。
- **IC 评分（Incremental Consistency Score）**：衡量 LLM 在相关推理任务中的一致性程度。

## 实验结果

### RBR 任务表现

- **平均准确率仅 44.4%**，LLM 在代码执行推理方面表现较差。
- **GPT-4-Turbo 表现最佳（75.0%），开源模型普遍低于 50%**。

### IC 评分

- **平均 IC 评分仅 10.3**，表明 LLM **逻辑不一致问题严重**。
- **GPT-4-Turbo 的 IC 评分（42.5）显著优于其他模型**。

### 代码推理能力 vs 代码生成

- **HumanEval 代码生成能力与 RBR/IC 任务相关性较高（>0.7）**。
- **不同 LLM 在代码生成和代码推理任务上的表现仍存在较大差异**。

## 结论

1. **当前 Code LLMs 在代码推理方面表现不佳**，尤其在理解代码执行路径和变量状态方面。
2. **LLM 存在严重的逻辑不一致性问题**，影响其在复杂代码任务中的可靠性。
3. **提升代码 LLM 需要新的训练方法**：
   - **结合代码执行行为进行训练**，增强 LLM 对代码运行时的理解。
   - **改进提示策略（如 Chain-of-Thought, Tree-of-Thoughts）**，提升 LLM 逻辑推理能力。

## 未来工作

- **探索更先进的推理框架**，提升 LLM 代码执行行为的理解能力。
- **改进评测方法**，使其更贴合实际软件开发场景。
- **提升 LLM 在复杂代码任务中的逻辑一致性**，使其更适用于自动化软件工程任务。
