---
comments: true
---

# [ICML] NExT: Teaching Large Language Models to Reason about Code Execution

???+ info

    - Author: Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, Pengcheng Yin
    - Conference: ICML 2024
    - arXiv: [2404.14662](https://arxiv.org/abs/2404.14662)

<div class="card file-block" markdown="1">
<div class="file-icon"><img src="/Note/assets/images/icons/pdf.svg" style="height: 3em;"></div>
<div class="file-body">
<div class="file-title">论文</div>
<div class="file-meta">1.56 MB / 35 P / 2025-03-17</div>
</div>
<a class="down-button" target="_blank" href="/Note/assets/files/science_research/arXiv_2404_14662.pdf" markdown="1">:fontawesome-solid-download: 下载</a>
</div>

## 研究背景与问题定义

### 研究背景

近年来，大型语言模型（LLMs, Large Language Models）在代码生成、代码解释、代码审查等任务中取得了显著进展。然而，尽管这些模型在生成高质量代码方面表现出色，它们在需要深入理解程序执行行为的复杂软件工程任务（如程序修复）中仍然存在局限性。

### 研究问题

如何使大型语言模型具备理解和推理程序执行过程的能力，从而提高其在代码修复任务中的表现？

### 关键挑战

1. **模型无法自然理解执行信息**：LLMs 对程序执行的变量状态缺乏理解，难以有效利用执行信息进行推理。
2. **执行轨迹的复杂性**：原始程序执行轨迹信息量大，直接编码会超出模型上下文窗口，且难以处理。
3. **推理合理性与正确性评估**：需要一种方法评估模型生成的推理过程是否合理，并确保其能够有效指导代码修复。

## NExT 方法概述

### 方法框架

NExT (Naturalized Execution Tuning) 通过自训练（Self-Training）方法，增强 LLMs 在代码修复任务中的执行推理能力。

**核心思想**：

1. 生成带有执行轨迹的自然语言推理（NL rationales）和修复代码。
2. 使用单元测试验证修复代码的正确性，筛选有效的推理和修复对。
3. 通过多轮微调增强模型的执行推理能力。

### 方法流程

给定程序修复任务数据集 $D = \{(x_j, \tilde{y}_j, T_j, \epsilon_j)\}$，NExT 采用以下流程：

1. **输入信息**：
    - 自然语言任务描述 $x$
    - 有缺陷代码 $\tilde{y}$
    - 测试用例集合 $T$
    - 执行轨迹 $\epsilon$
2. **采样 (Sampling)**：从当前模型 $P_\theta$ 生成一组候选推理 $r$ 和修复代码 $\hat{y}$。
3. **过滤 (Filtering)**：使用单元测试执行修复代码，保留通过所有测试的 $(r, \hat{y})$ 对。
4. **微调 (Training)**：对模型进行多轮微调，目标是最大化这些合理推理-修复对的生成概率。

## 实验设置与结果分析

### 数据集

- **Mbpp-R**：基于 MBPP 数据集扩展，包含 10,047 个训练任务和 1,468 个开发任务。
- **HeFix+**：来自 HumanEvalFix 数据集，扩展了更严格的测试用例，包含 164 个修复任务。

### 评估指标

1. **End-to-End Fix Rate (pass@k)**：表示在 $k$ 次生成中至少有一次生成的修复代码能通过所有测试用例的比例。
2. **Proxy-based Evaluation**：利用生成的推理过程作为提示，使用更小的 LLM 解决原始任务，以此评估推理质量。

### 主要结果

1. **修复成功率显著提升**
    - 在 Mbpp-R 上，PaLM 2-L 经过 NExT 微调后，pass@1 提升 **26.1%**。
    - 在 HeFix+ 上，PaLM 2-L+NExT pass@1 提升 **14.3%**。

2. **推理质量提升**
    - Proxy-based 评估表明，NExT 训练后的模型生成的推理对更小的 LLM 更具指导性。

3. **无需执行轨迹也能有效工作**
    - 在测试阶段去除执行轨迹输入时，NExT 训练的模型 pass@1 仍比原始模型高 **21.8%**。

## 论文贡献与未来工作

### 论文贡献

1. 提出了 NExT 方法，使 LLMs 能够通过执行轨迹进行自然语言推理，提升代码修复能力。
2. 设计了基于单元测试的弱监督自训练框架，避免繁琐的人工标注。
3. 证明了 NExT 方法在缺乏执行轨迹的情况下仍能有效工作。

### 未来工作

1. 将 NExT 扩展到更多编程语言和代码理解任务。
2. 探索更复杂的执行信息（如运行时环境、数据依赖关系）。
