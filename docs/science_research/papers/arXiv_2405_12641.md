---
comments: true
---

# [TSE] Fight Fire with Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks?

???+ info

    - Author: Xiao Yu, Lei Liu, **Xing Hu**, Jacky Wai Keung, Jin Liu, Xin Xia
    - Journal: IEEE Transactions on Software Engineering, Volume 50, Issue 12
    - arXiv: [2405.12641](https://arxiv.org/abs/2405.12641)

<div class="card file-block" markdown="1">
<div class="file-icon"><img src="/Note/assets/images/icons/pdf.svg" style="height: 3em;"></div>
<div class="file-body">
<div class="file-title">论文</div>
<div class="file-meta">1.44 MB / 19 P / 2025-02-27</div>
</div>
<a class="down-button" target="_blank" href="/Note/assets/files/science_research/arXiv_2405_12641.pdf" markdown="1">:fontawesome-solid-download: 下载</a>
</div>

这篇论文的核心研究问题是 ChatGPT 在源代码相关任务中的自我验证能力。具体来说，论文评估了 ChatGPT 在代码生成、代码补全和程序修复任务中的自我验证效果。

## 引言

研究的背景是随着像 ChatGPT 这样的语言模型在软件开发中的使用增多，验证其生成代码的质量变得尤为重要。以往研究提出了将 ChatGPT 用作开发者和测试者的多代理协作模型，但并未评估测试报告的有效性。

本文旨在通过实证研究，探讨 ChatGPT 在代码生成、补全和修复中的自我验证能力。研究提出了三个问题：

- ChatGPT 在代码生成中的自我验证能力如何？
- ChatGPT 在代码补全中的自我验证能力如何？
- ChatGPT 在程序修复中的自我验证能力如何？

## 研究设计

研究旨在评估 ChatGPT 在源代码相关任务中的自我验证能力，特别是在**代码生成**、**代码补全**和**程序修复**三个常见任务中的表现。为了深入探讨这些任务，研究设计了三种不同的验证方式：**直接问题** (Direct Question)、**引导性问题** (Guiding Question)和**测试报告** (Test Report)。

### 代码生成

数据集：选择了两个广泛使用的代码生成数据集，MBXP 和 HumanEval-X。这些数据集包含了大量的编程问题和相应的测试用例，用于评估 ChatGPT 在生成代码时的表现。

过程：

- Step 1：要求 ChatGPT 根据给定的任务描述生成代码。生成的代码不包含测试用例。
- Step 2：使用三种不同的验证方式：
    - 直接问题：询问 ChatGPT 生成的代码是否符合要求，得到 "是" 或 "否" 的答案。
    - 引导性问题：要求 ChatGPT 判断自己是否同意某些关于代码错误的断言。如果代码有误，要求 ChatGPT 返回 "是" 并给出解释。
    - 测试报告：要求 ChatGPT 生成一个测试报告来验证代码是否正确。
  
### 代码补全

数据集：采用了 Pearce 等人的数据集，其中包含了多种编码漏洞场景，专门用于测试代码补全时的漏洞识别能力。

过程：

- Step 1：要求 ChatGPT 补全不完整的代码，确保生成的代码不包含漏洞。
- Step 2：使用三种验证方式：
    - 直接问题：询问生成的代码是否包含漏洞。
    - 引导性问题：询问 ChatGPT 是否同意某些关于代码漏洞的断言。
    - 测试报告：要求 ChatGPT 生成测试报告，分析补全后的代码是否存在漏洞。
  
### 程序修复

数据集：选择了 QuixBugs 和 HumanEval-JavaR 数据集，这些数据集包含了具有已知错误的程序和对应的正确版本。

过程：

- Step 1：要求 ChatGPT 修复错误代码，确保修复后的代码无误。
- Step 2：使用三种验证方式：
    - 直接问题：询问修复后的代码是否无误。
    - 引导性问题：询问 ChatGPT 是否同意某些关于修复错误的断言。
    - 测试报告：要求 ChatGPT 生成测试报告，验证程序是否成功修复了错误。

## 实验结果

在实验中，研究人员对**代码生成**、**代码补全**和**程序修复**三个任务进行了详细评估，比较了三种自我验证方式的效果。每个任务都有其特定的结果。

### 代码生成

准确性和自我验证问题：

- 生成准确度：ChatGPT 在生成代码时，常常错误地认为生成的代码是正确的。特别是当通过**直接问题**提问时，ChatGPT 高估了代码的正确性。例如，对于 HumanEval-X 数据集，ChatGPT 对生成代码的预测准确度较高，但往往忽略了生成代码中的错误。
- 引导性问题：引导性问题在帮助 ChatGPT 识别错误代码方面表现得更好。例如，在 MBXP 数据集中，使用引导性问题比直接问题更能有效识别错误生成的代码，但也带来了更高的假阳性率（即将无错误的代码误判为错误）。
- 测试报告：测试报告的效果相对较差。尽管它可以帮助识别一些漏洞，但对于生成错误代码的解释却常常不准确，尤其是在生成代码中出现的逻辑错误时，测试报告往往无法给出准确的反馈。

### 代码补全

漏洞检测与自我验证：

- 直接问题：当询问 ChatGPT 代码补全是否有漏洞时，ChatGPT 很少正确识别漏洞。例如，直接问题提问时，ChatGPT 高估了补全代码的安全性，只有少数的补全漏洞被正确检测出来，导致回忆精度较低。
- 引导性问题：与直接问题相比，引导性问题显著提升了对漏洞的识别能力，帮助 ChatGPT 更好地发现补全中的漏洞，但增加了误报率。也就是说，更多的代码被误判为存在漏洞。
- 测试报告：使用测试报告时，ChatGPT 更准确地识别了漏洞，且给出的漏洞解释相对较为合理。尽管如此，测试报告仍然存在一些缺陷，尤其是在检测程序结构性漏洞方面。

### 程序修复

修复效果与自我验证：

- 直接问题：在程序修复任务中，ChatGPT 经常错误地认为它已经成功修复了代码。例如，在 QuixBugs-Python 数据集中的修复任务中，ChatGPT 错误地认为一些修复已经成功，但实际上修复并未解决问题。
- 引导性问题：引导性问题能够有效提升识别失败修复的能力，尤其是在代码修复失败的情况下。相比直接问题，引导性问题提供了更多的修复失败识别，尽管它也增加了误报的数量。
- 测试报告：测试报告同样存在类似的问题，能够识别出部分未修复的错误，但在大多数修复任务中，报告未能成功识别出所有错误。

### 自相矛盾的幻觉

论文还提到了一种有趣的现象：**自相矛盾的幻觉** (self-contradictory hallucinations)。这指的是 ChatGPT 在生成代码时，认为自己生成的代码是正确的，但在后续的自我验证中却认为其为错误代码。这种现象不仅影响了代码生成，还出现在代码补全和修复任务中。

> 在生成代码时，ChatGPT 有时会生成错误代码，但在后续验证时却判断其为正确代码。例如，在代码生成过程中，ChatGPT 一开始认为它的代码是正确的，但在生成的测试报告中却预测代码是错误的。

## 讨论与结论

自我验证能力的局限性：尽管 ChatGPT 在代码生成、补全和修复中展现出一定的能力，但它的自我验证功能存在许多局限，尤其是在准确性和一致性方面。ChatGPT 经常发生自相矛盾的“幻觉”，例如它初始认为生成的代码是正确的，但后续预测其为错误代码。

未来研究方向：研究建议，尽管 ChatGPT 目前无法独立承担代码验证的任务，但它可以作为开发者的辅助工具，协助完成一些重复性较高的工作。未来的研究可以致力于提升其在代码验证中的能力，减少自相矛盾的错误。
