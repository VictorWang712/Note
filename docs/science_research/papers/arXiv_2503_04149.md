---
comments: true
---

# [arXiv] Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination

???+ info

    - Author: Simin Chen, Pranav Pusarla, Baishakhi Ray
    - arXiv: [2503.04149](https://arxiv.org/abs/2503.04149)

<div class="card file-block" markdown="1">
<div class="file-icon"><img src="/Note/assets/images/icons/pdf.svg" style="height: 3em;"></div>
<div class="file-body">
<div class="file-title">论文</div>
<div class="file-meta">1.38 MB / 14 P / 2025-03-17</div>
</div>
<a class="down-button" target="_blank" href="/Note/assets/files/science_research/arXiv_2503_04149.pdf" markdown="1">:fontawesome-solid-download: 下载</a>
</div>

## 研究背景与问题定义

### 研究背景

- 代码大语言模型（Code LLMs）在代码生成、代码理解等任务中表现卓越，然而，现有的静态基准测试无法有效评估其真实推理能力。
- **数据污染（Data Contamination）** 是由于模型训练时使用了大量公开数据，可能会使模型在测试集上表现出虚假的高准确率。
- 现有方法如 LiveCodeBench 和 PPM 依赖大量人工设计，且生成问题的语义复杂性难以与原始问题保持一致。

### 研究目标

动态生成语义多样但复杂性一致的编程问题，准确评估代码大语言模型在数据污染环境下的推理能力。

## DyCodeEval 方法概述

### 设计理念

DyCodeEval 受到 **变形测试（Metamorphic Testing）** 的启发，旨在通过自动化方法生成复杂性不变但语义多样的编程问题。

### 核心思想

DyCodeEval 将编程问题分解为两部分：

1. **复杂性相关的算法抽象（Algorithmic Abstraction）**
2. **复杂性无关的上下文描述（Context Description）**

通过修改上下文描述生成多样化问题，保证复杂性不变，确保模型推理能力的公平评估。

## DyCodeEval 方法框架

### 系统架构

DyCodeEval 包含以下四个大语言模型驱动的智能体（Agents）：

1. **场景提议智能体（Scenario Proposer）**
   - 从预定义的领域（如银行、医疗、教育）生成多样化的场景池。

2. **上下文生成智能体（Context Generator）**
   - 基于输入变量类型生成与场景相关的上下文信息。

3. **提示重写智能体（Prompt Rewriter）**
   - 结合场景和上下文信息，重新表述种子问题以生成多样化的编程问题。

4. **验证智能体（Validator）**
   - 确保生成的新问题在语义上与原始问题一致，保持算法复杂性不变。

### 生成过程

1. 输入种子问题（Seed Problem）。
2. 通过 **场景提议智能体** 生成真实场景池。
3. 由 **上下文生成智能体** 推断输入类型并生成语境信息。
4. **提示重写智能体** 重写问题，生成语义多样但复杂性保持一致的新问题。
5. **验证智能体** 确保新问题的语义和复杂性与原问题一致。

## 数学定义与理论分析

### 碰撞分析（Collision Analysis）

**无重复生成概率下界**：若 DyCodeEval 对相同种子问题执行 $M + 1$ 次，且每次生成均独立，则第 $M$ 次生成与第 1 次生成不同的概率下界为：

$$
P \geq 1 - \exp\left( -\frac{M}{||S|| \times ||C|| - 1} \right)
$$

其中，$||S||$ 表示场景数量，$||C||$ 表示上下文数量。

**存在至少一个碰撞的概率上界**：

$$
P \leq 1 - \exp\left( -\frac{M^2 - M}{2 ||S|| \times ||C||} \right)
$$

## 实验设置与结果分析

### 数据集

- **HumanEval**：由 OpenAI 提供的代码生成基准测试数据集，包含 164 个 Python 编程问题。
- **MBPP-Sanitized**：由 Google 提供的 427 个 Python 编程问题，专用于代码生成模型的评估。

### 评估指标

- **Pass@K**：

$$
\text{Pass@K} = \mathbb{E}_{\text{Problems}} \left[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \right]
$$

其中，$n$ 是生成的候选解数量，$c$ 是通过测试用例的解的数量。

- **DivPass@K**（动态多样性评估指标）：

在 $K$ 个语义变体中，计算模型生成的代码片段通过所有测试用例的概率，能够更敏感地反映数据污染。

### 主要实验结果

1. **数据污染的影响**
   - 静态基准测试在数据污染下无法准确反映模型推理能力，导致 Pass@K 偏高。

2. **DyCodeEval 的有效性**
   - DyCodeEval 显著降低了模型在数据污染环境下的虚假性能提升，确保评估结果的稳定性。

3. **问题多样性**
   - DyCodeEval 生成的问题在语法和语义上多样性显著，超越其他基于规则的变异方法。

4. **评估结果的稳定性**
   - 多次运行 DyCodeEval，生成的基准测试结果方差极小，证明了方法的稳定性。

## 贡献与未来工作

### 论文贡献

1. 提出了一种动态基准测试方法 DyCodeEval，能有效缓解数据污染带来的评估偏差。
2. 设计了一套基于 LLM 的多智能体框架，自动生成语义多样但复杂性一致的编程问题。
3. 提出了新的动态评估指标 DivPass@K，准确衡量模型推理能力，特别是在数据污染环境下。

### 未来工作

1. 提升生成过程的计算效率，探索更轻量级的 LLM 作为基础模型。
2. 优化提示生成方式，减少冗余信息，增强问题表述的清晰度。
